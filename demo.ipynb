{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import soundfile\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechTranslatorApp:\n",
    "    def __init__(self):\n",
    "        model_path = 'Phi-4-multimodal-instruct'\n",
    "        \n",
    "        # Initialize models\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype='auto',\n",
    "            _attn_implementation='flash_attention_2',\n",
    "        ).cuda()\n",
    "        \n",
    "        self.generation_config = GenerationConfig.from_pretrained(model_path, 'generation_config.json')\n",
    "\n",
    "        # Define prompts\n",
    "        self.user_prompt = '<|user|>'\n",
    "        self.assistant_prompt = '<|assistant|>'\n",
    "        self.prompt_suffix = '<|end|>'\n",
    "        self.system_prompt = '<|system|>'\n",
    "        \n",
    "        # Define supported languages with their codes and native names\n",
    "        self.languages = {\n",
    "            \"English\": {\n",
    "                \"code\": \"en\",\n",
    "                \"native\": \"English\"\n",
    "            },\n",
    "            \"Chinese\": {\n",
    "                \"code\": \"zh\",\n",
    "                \"native\": \"中文\"\n",
    "            },\n",
    "            \"German\": {\n",
    "                \"code\": \"de\",\n",
    "                \"native\": \"Deutsch\"\n",
    "            },\n",
    "            \"French\": {\n",
    "                \"code\": \"fr\",\n",
    "                \"native\": \"Français\"\n",
    "            },\n",
    "            \"Italian\": {\n",
    "                \"code\": \"it\",\n",
    "                \"native\": \"Italiano\"\n",
    "            },\n",
    "            \"Japanese\": {\n",
    "                \"code\": \"ja\",\n",
    "                \"native\": \"日本語\"\n",
    "            },\n",
    "            \"Spanish\": {\n",
    "                \"code\": \"es\",\n",
    "                \"native\": \"Español\"\n",
    "            },\n",
    "            \"Portuguese\": {\n",
    "                \"code\": \"pt\",\n",
    "                \"native\": \"Português\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.translations_dir = \"translations\"\n",
    "        os.makedirs(self.translations_dir, exist_ok=True)\n",
    "        self.translations = self.load_translations()\n",
    "        \n",
    "    def get_translation_file_path(self, lang_code):\n",
    "        \"\"\"Get path for language-specific translation file\"\"\"\n",
    "        return os.path.join(self.translations_dir, f\"translations_{lang_code}.json\")\n",
    "\n",
    "    def load_translations(self):\n",
    "        \"\"\"Load translations for all languages\"\"\"\n",
    "        translations = {}\n",
    "        for lang_info in self.languages.values():\n",
    "            file_path = self.get_translation_file_path(lang_info[\"code\"])\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    translations[lang_info[\"code\"]] = json.load(f)\n",
    "            else:\n",
    "                translations[lang_info[\"code\"]] = []\n",
    "        return translations\n",
    "\n",
    "    def save_translation(self, lang_code, translation):\n",
    "        \"\"\"Save translation for specific language\"\"\"\n",
    "        file_path = self.get_translation_file_path(lang_code)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(translation, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def transcribe_audio(self, audio_input, source_lang=\"English\"):\n",
    "        \"\"\"Transcribe audio to text\"\"\"\n",
    "        speech_prompt = f\"Transcribe this {source_lang} audio to text.\"\n",
    "        \n",
    "        prompt = f'{self.user_prompt}<|audio_1|>{speech_prompt}{self.prompt_suffix}{self.assistant_prompt}'\n",
    "        audio = soundfile.read(audio_input)\n",
    "        \n",
    "        inputs = self.processor(text=prompt, audios=[audio], return_tensors='pt').to('cuda')\n",
    "        \n",
    "        generate_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            generation_config=self.generation_config,\n",
    "        )\n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        \n",
    "        transcription = self.processor.batch_decode(\n",
    "            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        return transcription.strip()\n",
    "\n",
    "    def translate_text(self, text, source_lang, target_lang):\n",
    "        \"\"\"Translate text between languages\"\"\"\n",
    "        if not text:\n",
    "            return \"No text to translate\"\n",
    "        \n",
    "        translation_prompt = f\"Translate the following {source_lang} text to {target_lang}. Provide only the translation without any additional text or explanation:\"\n",
    "        \n",
    "        prompt = f'{self.system_prompt}You are a professional translator.{self.prompt_suffix}{self.user_prompt}{translation_prompt}\\n\\n{text}{self.prompt_suffix}{self.assistant_prompt}'\n",
    "\n",
    "        inputs = self.processor(prompt, images=None, return_tensors='pt').to('cuda')\n",
    "\n",
    "        try:\n",
    "            generate_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=2000,\n",
    "                generation_config=self.generation_config,\n",
    "            )\n",
    "            generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "            translation = self.processor.batch_decode(\n",
    "                generate_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            return translation.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {str(e)}\")\n",
    "            return f\"Translation failed: {str(e)}\"\n",
    "\n",
    "    def process_translation(self, audio, source_lang, target_lang):\n",
    "        \"\"\"Process audio input and generate translation\"\"\"\n",
    "        # Transcribe audio to text\n",
    "        source_text = self.transcribe_audio(audio, source_lang)\n",
    "        \n",
    "        # Translate to target language\n",
    "        translation = self.translate_text(source_text, source_lang, target_lang)\n",
    "        \n",
    "        # Create translation entry\n",
    "        translation_entry = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"source_language\": source_lang,\n",
    "            \"target_language\": target_lang,\n",
    "            \"source_text\": source_text,\n",
    "            \"translated_text\": translation\n",
    "        }\n",
    "        \n",
    "        # Save translation\n",
    "        source_code = self.languages[source_lang][\"code\"]\n",
    "        target_code = self.languages[target_lang][\"code\"]\n",
    "        \n",
    "        if source_code not in self.translations:\n",
    "            self.translations[source_code] = []\n",
    "        if target_code not in self.translations:\n",
    "            self.translations[target_code] = []\n",
    "            \n",
    "        self.translations[source_code].append(translation_entry)\n",
    "        self.translations[target_code].append(translation_entry)\n",
    "        \n",
    "        self.save_translation(source_code, self.translations[source_code])\n",
    "        self.save_translation(target_code, self.translations[target_code])\n",
    "        \n",
    "        return self.format_translation_display(translation_entry)\n",
    "\n",
    "    def format_translation_display(self, entry):\n",
    "        \"\"\"Format translation for display\"\"\"\n",
    "        output = f\"\"\"Timestamp: {entry['timestamp']}\\n\\n\"\"\"\n",
    "        output += f\"\"\"Source Language ({entry['source_language']}):\\n{entry['source_text']}\\n\\n\"\"\"\n",
    "        output += f\"\"\"Target Language ({entry['target_language']}):\\n{entry['translated_text']}\\n\"\"\"\n",
    "        return output\n",
    "\n",
    "    def list_translations(self, lang_code):\n",
    "        \"\"\"List translations for specific language\"\"\"\n",
    "        if lang_code not in self.translations or not self.translations[lang_code]:\n",
    "            return \"No translations found\"\n",
    "        \n",
    "        return \"\\n\\n---\\n\\n\".join([\n",
    "            self.format_translation_display(entry) \n",
    "            for entry in self.translations[lang_code]\n",
    "        ])\n",
    "\n",
    "    def create_interface(self):\n",
    "        \"\"\"Create Gradio interface\"\"\"\n",
    "        with gr.Blocks(theme=gr.themes.Soft()) as interface:\n",
    "            gr.Markdown(\"# Multilingual Speech Translation Hub\")\n",
    "            gr.Markdown(\"Record speech or upload audio file for translation between multiple languages\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                source_lang = gr.Dropdown(\n",
    "                    choices=list(self.languages.keys()),\n",
    "                    value=\"English\",\n",
    "                    label=\"Source Language\"\n",
    "                )\n",
    "                target_lang = gr.Dropdown(\n",
    "                    choices=list(self.languages.keys()),\n",
    "                    value=\"Chinese\",\n",
    "                    label=\"Target Language\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                audio_input = gr.Audio(\n",
    "                    sources=[\"microphone\", \"upload\"],\n",
    "                    type=\"filepath\",\n",
    "                    label=\"Record or Upload Audio\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                translate_btn = gr.Button(\"Translate\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                output = gr.Textbox(\n",
    "                    label=\"Translation Results\",\n",
    "                    lines=10\n",
    "                )\n",
    "            \n",
    "            # History viewer\n",
    "            with gr.Accordion(\"Translation History\", open=False):\n",
    "                lang_select = gr.Dropdown(\n",
    "                    choices=list(self.languages.keys()),\n",
    "                    value=\"English\",\n",
    "                    label=\"Select Language\"\n",
    "                )\n",
    "                history_output = gr.Textbox(\n",
    "                    label=\"Translation History\",\n",
    "                    lines=20\n",
    "                )\n",
    "            \n",
    "            # Event handlers\n",
    "            translate_btn.click(\n",
    "                fn=self.process_translation,\n",
    "                inputs=[audio_input, source_lang, target_lang],\n",
    "                outputs=output\n",
    "            )\n",
    "            \n",
    "            lang_select.change(\n",
    "                fn=lambda x: self.list_translations(self.languages[x][\"code\"]),\n",
    "                inputs=[lang_select],\n",
    "                outputs=history_output\n",
    "            )\n",
    "            \n",
    "            return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_app():\n",
    "    # Create app instance\n",
    "    app = SpeechTranslatorApp()\n",
    "    \n",
    "    # Launch Gradio interface\n",
    "    interface = app.create_interface()\n",
    "    interface.launch(\n",
    "        share=True,\n",
    "        server_name=\"0.0.0.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrun_app\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_app\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Create app instance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     app = \u001b[43mSpeechTranslatorApp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Launch Gradio interface\u001b[39;00m\n\u001b[32m      6\u001b[39m     interface = app.create_interface()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mSpeechTranslatorApp.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize models\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.cuda()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.generation_config = GenerationConfig.from_pretrained(model_path, \u001b[33m'\u001b[39m\u001b[33mgeneration_config.json\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Define prompts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    563\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    568\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\transformers\\modeling_utils.py:4336\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4334\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m4336\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\n\u001b[32m   4338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4341\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m   4342\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\transformers\\modeling_utils.py:2109\u001b[39m, in \u001b[36mPreTrainedModel._autoset_attn_implementation\u001b[39m\u001b[34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[39m\n\u001b[32m   2106\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2109\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflex_attention\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2117\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._check_and_enable_flex_attn(config, hard_check_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ian\\miniconda3\\envs\\2brain\\Lib\\site-packages\\transformers\\modeling_utils.py:2252\u001b[39m, in \u001b[36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[39m\n\u001b[32m   2250\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m   2251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2252\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2254\u001b[39m flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda:\n",
      "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "run_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível? True\n",
      "GPU: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA disponível?\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Nenhuma GPU detectada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ian\\miniconda3\\envs\\2brain2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ian\\.cache\\huggingface\\hub\\models--deepseek-ai--deepseek-llm-7b-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/deepseek-llm-7b-base\", trust_remote_code=True)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response:\n",
      "{'id': 'chatcmpl-nk0pcrsda5986916rocs9b', 'object': 'chat.completion', 'created': 1746757836, 'model': 'deepseek-r1-distill-qwen-7b', 'choices': [{'index': 0, 'logprobs': None, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': '<think>\\nOkay, so I need to figure out how to organize this text about sustainable living. The user has given a pretty detailed response with various sections like introduction, reducing waste, energy efficiency, water conservation, carbon footprint, mental health, and future trends.\\n\\nHmm, first, I should probably read through the existing content to understand what\\'s covered and where there might be gaps or opportunities for expansion. Let me start by skimming each section.\\n\\nThe introduction is good; it sets up sustainable living as a way to live in harmony with nature. It mentions reducing waste, conserving energy, water, and protecting the environment. That\\'s solid. Maybe I can add more about why sustainability matters beyond just environmental impact—like health benefits or economic reasons.\\n\\nIn the \"Reducing Waste\" section, it talks about composting organic waste and recycling plastics. Perhaps I should include information on proper segregation at home, maybe give examples of what goes where. Also, maybe touch on the challenges people face in maintaining a zero-waste lifestyle, like lack of knowledge or infrastructure issues.\\n\\nThe \"Energy Efficiency\" part mentions turning off lights when not in use and using LED bulbs. That\\'s good, but could include more specific tips, like adjusting thermostats for efficiency, using public transport instead of cars, or investing in renewable energy sources like solar panels.\\n\\nWater conservation is covered with greywater recycling and rainwater harvesting. Maybe expand on how to set up a greywater system step-by-step or discuss the benefits beyond just water savings—like saving money or reducing chemical usage.\\n\\nThe \"Carbon Footprint\" section has some good points about transportation and diet, but could include more actionable steps like switching to electric vehicles or adopting a plant-based diet. Maybe also touch on sustainable fashion options as an additional point here.\\n\\nMental health is addressed by talking about stress reduction through meditation. It\\'s important, but perhaps adding other aspects like increased job satisfaction from sustainable practices or community building could provide more depth.\\n\\nFor future trends, the current points are good, but maybe include how technology might play a bigger role in sustainability—like smart home devices or AI-driven waste management systems.\\n\\nI should also consider the flow of the text. It seems to move logically from introduction through each area, so maintaining that order makes sense. Maybe add subheadings within each section for better organization, like \"Proper Segregation of Waste\" under Reduce Waste, breaking down into organic and recyclable items with examples.\\n\\nAnother thought: including case studies or success stories could make the content more relatable and credible. For example, mention specific cities or individuals who have successfully reduced their carbon footprint through sustainable practices.\\n\\nI should also think about target audience. If it\\'s for a general readership, keeping language accessible but informative is key. Avoid jargon unless necessary. Maybe add tips for different lifestyles—like how someone living in an apartment can still practice sustainability effectively.\\n\\nLastly, ensuring consistency in information across sections. For example, mentioning recycling rates or the impact of each action taken consistently will reinforce the message throughout.\\n\\nSo putting it all together, I should enhance each section by adding more details, examples, and maybe even some actionable steps. Also, consider including additional points that tie back to why sustainability is important beyond just the environment, like health benefits or economic impacts.\\n</think>\\n\\n**Organized Content on Sustainable Living: An Enhanced Overview**\\n\\n---\\n\\n### Introduction\\nSustainable living represents a holistic approach to living in harmony with nature, emphasizing environmental stewardship and individual well-being. Beyond mitigating ecological impact, sustainable practices offer health benefits through reduced pollution exposure and may even enhance productivity by optimizing resource use.\\n\\n---\\n\\n### Reducing Waste\\n\\n- **Proper Segregation of Waste**\\n  - **Organic Waste:** Compost kitchen scraps to enrich the soil.\\n  - **Recyclables:** Separate paper, plastic, glass, and metal. Use specific containers or apps for tracking.\\n  \\n- **Waste Management Challenges**\\n  - Highlight barriers like lack of education or infrastructure issues, suggesting community support as a solution.\\n\\n---\\n\\n### Energy Efficiency\\n\\n- **Home Tips**\\n  - Adjust lighting schedules to save energy.\\n  - Invest in LED bulbs and smart home devices for convenience.\\n  - Optimize thermostats based on weather conditions.\\n\\n- **Lifestyle Choices**\\n  - Encourage carpooling or public transport to reduce energy consumption.\\n  - Consider solar panel installation for renewable energy.\\n\\n---\\n\\n### Water Conservation\\n\\n- **Greywater Recycling**\\n  - Provide step-by-step guides on setting up greywater systems, including reuse of water from showers and laundry.\\n\\n- **Rainwater Harvesting**\\n  - Discuss the benefits beyond water savings, such as cost reduction and reduced chemical usage.\\n  - Include practical tips for installation in different climates.\\n\\n---\\n\\n### Carbon Footprint\\n\\n- **Transportation Choices**\\n  - Emphasize electric vehicles (EVs) and public transport alternatives.\\n  \\n- **Dietary Impact**\\n  - Highlight plant-based diets or vegan options, linking nutritional value to sustainability.\\n\\n---\\n\\n### Mental Health\\n\\n- **Well-being Benefits**\\n  - Expand on increased job satisfaction from sustainable practices and the role of community in mental health.\\n\\n---\\n\\n### Future Trends\\n\\n- **Technology Integration**\\n  - Discuss AI-driven waste management and smart home devices enhancing sustainability.\\n  \\n- **Sustainable Fashion**\\n  - Explore eco-friendly clothing options as part of a broader sustainable lifestyle.\\n\\n---\\n\\n### Conclusion\\nSustainable living is not just about environmental impact but also about personal well-being. By adopting these practices, individuals can contribute positively to both their communities and the planet. Encouraging case studies or success stories can further inspire and validate these efforts.\\n\\n---\\n\\nThis structured approach ensures each aspect of sustainable living is addressed with depth and actionable insights, catering to a general audience while maintaining an accessible and informed tone.'}}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 1170, 'total_tokens': 1198}, 'stats': {}, 'system_fingerprint': 'deepseek-r1-distill-qwen-7b'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"deepseek-r1-distill-qwen-7b\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Organize the content, in any way possible based on the subject, add content if possible, give ideas, insights, and suggestions\"\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": -1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ian\\miniconda3\\envs\\2brain2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ian\\miniconda3\\envs\\2brain2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ian\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-R1-Distill-Qwen-14B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [22:44<00:00, 341.19s/it]   \n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [02:01<06:02, 120.92s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", trust_remote_code=True)\n",
    "pipe(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2brain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
